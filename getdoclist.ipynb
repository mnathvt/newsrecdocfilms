{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "import string\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_info = requests.get('https://en.wikipedia.org/wiki/List_of_documentary_films').text\n",
    "soup = BeautifulSoup(web_info, 'lxml')\n",
    "#print(soup.prettify())\n",
    "list_table = soup.findAll('table', {'class': 'wikitable sortable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the names you think/know the section might be called\n",
    "possibles = ['Plot','Synopsis','Plot synopsis','Plot summary', \n",
    "             'Story','Plotline','The Beginning','Summary',\n",
    "            'Content','Premise', 'Overview', 'Details']\n",
    "\n",
    "# sometimes those names have 'Edit' latched onto the end due to \n",
    "# user error on Wikipedia. In that case, it will be 'PlotEdit'\n",
    "# so it's easiest just to make another list that acccounts for that\n",
    "possibles_edit = [i + 'Edit' for i in possibles]\n",
    "#then merge those two lists together\n",
    "all_possibles = possibles + possibles_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_list_num():\n",
    "    doc_list = {}\n",
    "\n",
    "    tab = list_table[0].findAll('i')\n",
    "    for i in range(len(tab)):\n",
    "        for node in tab[i].findAll('a'):\n",
    "            for idx in list(string.digits):\n",
    "                if ''.join(node.findAll(text = True)).startswith(idx):\n",
    "                    name = ''.join(node.findAll(text = True))\n",
    "                    #links.append('https://www.wikipedia.org'+node.get('href'))\n",
    "\n",
    "                    p = wikipedia.page(name).sections[0]\n",
    "                    if p in all_possibles:\n",
    "                        doc_list[name] = wikipedia.page(name).section(p).replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "                    else:\n",
    "                        doc_list[name] = wikipedia.page(name).summary.replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "    return doc_list\n",
    "\n",
    "df = pd.DataFrame(list(get_doc_list_num().items()), columns = ['Title', 'Description'])\n",
    "\n",
    "df.to_csv('doc_list/list_try.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_list_alph(start):\n",
    "    doc_list = {}\n",
    "    \n",
    "    tab = list_table[start].findAll('i')\n",
    "    \n",
    "    for i in range(len(tab)):\n",
    "        for node in tab[i].findAll('a'):\n",
    "            if ''.join(node.findAll(text = True)).startswith(string.ascii_uppercase[start - 1]):\n",
    "                name = ''.join(node.findAll(text = True))\n",
    "                #link = 'https://www.wikipedia.org'+node.get('href')\n",
    "                wiki_name = ''.join([name, ' (film)'])\n",
    "                \n",
    "                try:\n",
    "                    p = wikipedia.page(wiki_name).sections[0]\n",
    "                    #print(wiki_name, p)\n",
    "                    if p in all_possibles:\n",
    "                        doc_list[name] = wikipedia.page(wiki_name).section(p).replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "                    else:\n",
    "                        doc_list[name] = wikipedia.page(wiki_name).summary.replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                \n",
    "                except wikipedia.exceptions.DisambiguationError:\n",
    "                    #pass\n",
    "                    print(wiki_name)\n",
    "                except wikipedia.exceptions.PageError:\n",
    "                    pass           \n",
    "                    \n",
    "    return doc_list\n",
    "\n",
    "df = pd.DataFrame(list(get_doc_list_alph(7).items()), columns = ['Title', 'Description'])\n",
    "\n",
    "df.to_csv('doc_list/list_g.csv', header = True, index = False)\n",
    "\n",
    "\n",
    "\n",
    "#get_doc_list_alph(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list_table[26].findAll('i')\n",
    "n = []\n",
    "for i in range(len(t)):\n",
    "    for node in t[i].findAll('a'):\n",
    "        if ''.join(node.findAll(text = True)).startswith('Z'):\n",
    "            n.append(''.join(node.findAll(text = True)))\n",
    "print(len(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l_list():\n",
    "    \n",
    "    l_list = ['Life on Earth', 'The Living Planet', 'The Trials of Life', 'Life in the Freezer',\n",
    "        'The Private Life of Plants', 'The Life of Birds', \n",
    "        'The Life of Mammals', 'Life in the Undergrowth', 'Life in Cold Blood']\n",
    "\n",
    "    doc_list = {}\n",
    "    \n",
    "    for j in l_list:\n",
    "        wiki_name = ''.join([j, ' (film)'])\n",
    "        #print(wiki_name)\n",
    "        try:\n",
    "            p = wikipedia.page(wiki_name).sections[0]\n",
    "            #print(wiki_name, p)\n",
    "            if p in all_possibles:\n",
    "                doc_list[wiki_name.split('(')[0]] = wikipedia.page(wiki_name).section(p).replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "            else:\n",
    "                doc_list[wiki_name.split('(')[0]] = wikipedia.page(wiki_name).summary.replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            #pass\n",
    "            print(wiki_name)\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            pass           \n",
    "                    \n",
    "    return doc_list\n",
    "\n",
    "df = pd.DataFrame(list(get_l_list().items()), columns = ['Title', 'Description'])\n",
    "\n",
    "df.to_csv('doc_list/list_missl.csv', header = True, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "missing_names = ['Amarillo by Morning', 'Beyond Belief', 'Call + Response', 'Castro Street',\n",
    "                'Dust to Dust: The Health Effects of 9/11 (film)',\n",
    "                'Finders Keepers',\n",
    "                'Midnight Movies: From the Margin to the Mainstream',\n",
    "                'Million Calorie March: The Movie', 'No Place Like Home', 'The Outsider',\n",
    "                'Operation Wedding: My parents & the plane hijacking', 'Primary',\n",
    "                'Poetry in Motion', 'The Roosevelts (PBS)', 'Red String, The',\n",
    "                'Rolling Papers (film)', 'South of the Border', 'Surplus']\n",
    "print(len(missing_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amarillo by morning is a 1997 documentary short by spike jonze. while filming pro bullriders for a commercial at the national rodeo in the houston astrodome in houston, texas, spike befriended two suburban teenagers who aspired to be cowboys. the documentary chronicles an afternoon in their lives.\n",
      "amarillo by morning may refer to:\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'Amarillo by Morning (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        print(soup.find('p').text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the film follows the story of a san francisco pentecostal minister richard gazowsky on his quest to shoot a groundbreaking fantasy film called gravity: the shadow of joseph (described by him as \"star wars meets the ten commandments\"). the film follows him and members of his church as they go through preproduction and fly to alberobello, italy, for initial shooting that turns out to be marred with difficulties. after returning home, gazowsky manages to arrange a lease of the treasure island film studio, but as their promised financing from german investors never materializes, they get evicted and eventually sued by the city of san francisco for not paying their rent.\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'Audience of One (2007 film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 1, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "print(soup.findAll('p')[2].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beyond belief is a feature documentary directed by beth murphy. the film follows susan retik and patti quigley, two women who lost their husbands on september 11, 2001, as they set up humanitarian programs for war widows in afghanistan. it premiered at the 2007 tribeca film festival.[1]\n",
      "beyond belief may refer to:\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'Beyond Belief (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        print(soup.find('p').text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "castro street (1966) is a visual nonstory documentary film which inspired by satie[2] uses the sounds and sights of a city street—in this case, castro street near the standard oil refinery in richmond, california complete with diesel trains and gas plants[3]—to convey the streets own mood and feel. there is no dialogue in this non-narrative experimental film. it was directed by bruce baillie.\n",
      "\n",
      "castro street may refer to:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'Castro Street (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        print(soup.find('p').text.replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finders keepers is a 2015 documentary film by bryan carberry and clay tweel.[1] the story details john woods attempts to recover his mummified leg from shannon whisnant, after whisnant found the leg in a grill purchased at a storage unit auction.\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'Finders Keepers (2015 film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 1, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "print(soup.find('p').text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no place like home features a moving soundtrack made by musicians personally connected with the subjects and interviews with friends who survived katrina in new orleans. all proceeds from dvd sales go directly towards gulf coast restoration funds.[1][2]\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'No Place Like Home (2006 film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 1, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        #print(i)\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "print(soup.findAll('p')[2].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtitled \"a film about james toback,\" the outsider alternates among on-location shots, interviews with toback and with many of his film collaborators and celebrity friends, and commentary by film critic roger ebert. jarecki follows toback into the editing room after the initial shooting of when will i be loved. for this film, toback says he \"shot freely,\" so that \"all sorts of options are now open.\" toback describes film editing as a creative process on a par with acting or writing. \"editing is the making of the movie.\" screenwriter and director robert towne accepts tobacks invitation to a screening of the first production cut of when will i be loved. accepting townes critique that a principal character in the film lacks adequate introduction, toback then orders a re-shoot, adding two scenes to the final cut he shops to distributors.\n",
      "the movie received mostly positive reviews from critics, and performed well at the box office, grossing $33 million on a $10 million budget.\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'The Outsider (2005 film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        print(soup.findAll('p')[2].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling papers is a 2015 documentary film directed by mitch dickman and featuring ricardo baca.[1][2][3] the \"deceptively rote title\" is said to be a winking reference to bacas work as a newspaper writer.[4]\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'Rolling Papers (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 1, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "    print(soup.find('p').text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surplus: terrorized into being consumers is an award-winning 2003 swedish documentary film on consumerism and globalization, created by director erik gandini and editor johan söderberg. it looks at the arguments for capitalism and technology, such as greater efficiency, more time and less work, and argues that these are not being fulfilled, and they never will be. the film leans towards anarcho-primitivist ideology and argues for a simple and fulfilling life.\n"
     ]
    }
   ],
   "source": [
    "wiki_name = 'Surplus Terrorized into Being Consumers (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "print(soup.find('p').text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', ''))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "doc_list = {}\n",
    "\n",
    "wiki_name = 'Call + Response (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.findAll('p')[0].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '') + \\\n",
    "        soup.findAll('p')[1].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '') + \\\n",
    "        soup.findAll('p')[2].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "\n",
    "wiki_name = 'Midnight Movies: From the Margin to the Mainstream (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.findAll('p')[0].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '') + \\\n",
    "        soup.findAll('p')[1].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '') + \\\n",
    "        soup.findAll('p')[2].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "\n",
    "wiki_name = 'Million Calorie March: The Movie (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.findAll('p')[1].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "        \n",
    "\n",
    "wiki_name = 'Operation Wedding (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.findAll('p')[1].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "\n",
    "wiki_name = 'Primary (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.findAll('p')[1].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "\n",
    "wiki_name = 'Poetry in Motion (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.findAll('p')[1].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')    \n",
    "\n",
    "\n",
    "wiki_name = 'The Roosevelts (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.find('p').text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "\n",
    "wiki_name = 'The Red String (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.find('p').text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "        \n",
    "wiki_name = 'South of the Border (film) wiki'\n",
    "for i in search(wiki_name, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        soup = BeautifulSoup(requests.get(i).text, 'lxml')\n",
    "        doc_list[wiki_name.split('(')[0]] = \\\n",
    "        soup.findAll('p')[2].text.replace('\\n','').replace(\"\\'\",'').lower().replace(r'[^0-9.]+', '')\n",
    "\n",
    "\n",
    "#print(len(doc_list.keys()))\n",
    "\n",
    "df = pd.DataFrame(list(doc_list.items()),  columns = ['Title', 'Description'])\n",
    "df.to_csv('doc_list/list_missing.csv', header = True, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = {}\n",
    "rem_list = {}\n",
    "\n",
    "tab = list_table[0].findAll('i')   \n",
    "for i in range(len(tab)):\n",
    "    for node in tab[i].findAll('a'):\n",
    "        if ''.join(node.findAll(text = True)).startswith('The'):\n",
    "            name = ''.join(node.findAll(text = True))\n",
    "            #links.append('https://www.wikipedia.org'+node.get('href'))\n",
    "\n",
    "            p = wikipedia.page(name).sections[0]\n",
    "            if p in all_possibles:\n",
    "                doc_list[name] = wikipedia.page(name).section(p).replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "            else:\n",
    "                doc_list[name] = wikipedia.page(name).summary.replace('\\n','').replace(\"\\'\",\"\").lower().replace(r'[^0-9.]+', '')\n",
    "                \n",
    "rem_list.update(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(rem_list.items()),  columns = ['Title', 'Description'])\n",
    "df.to_csv('doc_list/list_rem.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_set = pd.read_csv('ml-20m/movies.csv')\n",
    "whole_set.loc[whole_set['genres'] == 'Documentary']['title']\n",
    "d = []\n",
    "for i in whole_set.loc[whole_set['genres'] == 'Documentary']['title']:\n",
    "    d.append(i.split('(')[0])\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Trials of Henry Kissinger, The _film'\n",
    "for i in search(query, tld = 'com', lang = 'en', num = 2, start = 0, stop = 1, pause = 2):\n",
    "    if 'https://en.wikipedia.org/wiki/' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
